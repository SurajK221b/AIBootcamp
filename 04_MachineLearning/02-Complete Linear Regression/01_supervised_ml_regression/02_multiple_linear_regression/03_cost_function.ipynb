{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "609b5e5c",
   "metadata": {},
   "source": [
    "### Cost Function: Definition, Intuition & Examples\n",
    "\n",
    "| **Aspect**             | **Details**                                                                                                                                                                                                                                  |\n",
    "| ---------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Definition**         | A **Cost Function** (also known as a **Loss Function**) is a mathematical function used to evaluate the performance of a machine learning model by quantifying the error between predicted and actual outputs.                               |\n",
    "| **Objective**          | Minimize the cost to improve the model's predictive accuracy during training. Lower cost implies better performance.                                                                                                                         |\n",
    "| **Mathematical Form**  | Let $\\hat{y}_i$ be the predicted output and $y_i$ be the actual output. For $n$ samples:<br>**MSE (Mean Squared Error):**<br>$J(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2$                                                    |\n",
    "| **Types (Examples)**   | • **MSE (Mean Squared Error)** – Regression<br>• **MAE (Mean Absolute Error)** – Regression<br>• **Binary Cross-Entropy** – Binary Classification<br>• **Categorical Cross-Entropy** – Multi-class Classification<br>• **Hinge Loss** – SVMs |\n",
    "| **Use Case Examples**  | • Predicting house prices (MSE)<br>• Image classification (Cross-Entropy)<br>• Sentiment analysis (Binary Cross-Entropy)                                                                                                                     |\n",
    "| **Intuition**          | The cost function acts like a **compass**, guiding the optimization algorithm (e.g., Gradient Descent) on how to update model parameters to reduce prediction error.                                                                         |\n",
    "| **Role in Training**   | During each epoch of training, the optimizer calculates the gradient of the cost function w\\.r.t. model parameters and updates the weights to minimize the cost.                                                                             |\n",
    "| **Visualization**      | Often represented as a **convex curve** for simple models; the global minimum corresponds to the optimal parameter values.                                                                                                                   |\n",
    "| **Code Example (MSE)** | `python<br>import numpy as np<br>def mse(y_true, y_pred):<br>    return np.mean((y_true - y_pred) ** 2)<br>`                                                                                                                                 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f230393a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.375\n"
     ]
    }
   ],
   "source": [
    "### Example for cost function in Python - Mean Squared Error (MSE) – Regression\n",
    "# This script defines a simple cost function that calculates the mean squared error\n",
    "# between predicted and actual values.\n",
    "import numpy as np\n",
    "def cost_function(predicted, actual):\n",
    "    \"\"\"\n",
    "    Calculate the mean squared error between predicted and actual values.\n",
    "\n",
    "    Parameters:\n",
    "    predicted (np.ndarray): Predicted values.\n",
    "    actual (np.ndarray): Actual values.\n",
    "\n",
    "    Returns:\n",
    "    float: Mean squared error.\n",
    "    \"\"\"\n",
    "    if len(predicted) != len(actual):\n",
    "        raise ValueError(\"Predicted and actual arrays must have the same length.\")\n",
    "    \n",
    "    mse = np.mean((predicted - actual) ** 2)\n",
    "    return mse\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data\n",
    "    predicted_values = np.array([3.0, -0.5, 2.0, 7.0])\n",
    "    actual_values = np.array([2.5, 0.0, 2.0, 8.0])\n",
    "    \n",
    "    # Calculate cost\n",
    "    cost = cost_function(predicted_values, actual_values)\n",
    "    print(f\"Mean Squared Error: {cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d7974ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.5\n"
     ]
    }
   ],
   "source": [
    "## Example Code Snippet for cost function in Python - Mean Absolute Error (MAE) – Regression\n",
    "import numpy as np\n",
    "def cost_function_mae(predicted, actual):\n",
    "    \"\"\"\n",
    "    Calculate the mean absolute error between predicted and actual values.\n",
    "\n",
    "    Parameters:\n",
    "    predicted (np.ndarray): Predicted values.\n",
    "    actual (np.ndarray): Actual values.\n",
    "\n",
    "    Returns:\n",
    "    float: Mean absolute error.\n",
    "    \"\"\"\n",
    "    if len(predicted) != len(actual):\n",
    "        raise ValueError(\"Predicted and actual arrays must have the same length.\")\n",
    "    \n",
    "    mae = np.mean(np.abs(predicted - actual))\n",
    "    return mae\n",
    "\n",
    "# Example usage for MAE\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data\n",
    "    predicted_values = np.array([3.0, -0.5, 2.0, 7.0])\n",
    "    actual_values = np.array([2.5, 0.0, 2.0, 8.0])\n",
    "    \n",
    "    # Calculate cost\n",
    "    cost_mae = cost_function_mae(predicted_values, actual_values)\n",
    "    print(f\"Mean Absolute Error: {cost_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8cf3324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Huber Loss: 0.1875\n"
     ]
    }
   ],
   "source": [
    "## Code Snippet for cost function in Python - Huber Loss – Regression\n",
    "import numpy as np\n",
    "def cost_function_huber(predicted, actual, delta=1.0):\n",
    "    \"\"\"\n",
    "    Calculate the Huber loss between predicted and actual values.\n",
    "\n",
    "    Parameters:\n",
    "    predicted (np.ndarray): Predicted values.\n",
    "    actual (np.ndarray): Actual values.\n",
    "    delta (float): Threshold parameter for Huber loss.\n",
    "\n",
    "    Returns:\n",
    "    float: Huber loss.\n",
    "    \"\"\"\n",
    "    if len(predicted) != len(actual):\n",
    "        raise ValueError(\"Predicted and actual arrays must have the same length.\")\n",
    "    \n",
    "    error = predicted - actual\n",
    "    is_small_error = np.abs(error) <= delta\n",
    "    squared_loss = 0.5 * error[is_small_error] ** 2\n",
    "    linear_loss = delta * (np.abs(error[~is_small_error]) - 0.5 * delta)\n",
    "    \n",
    "    huber_loss = np.mean(np.concatenate((squared_loss, linear_loss)))\n",
    "    return huber_loss\n",
    "\n",
    "# Example usage for Huber Loss\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data\n",
    "    predicted_values = np.array([3.0, -0.5, 2.0, 7.0])\n",
    "    actual_values = np.array([2.5, 0.0, 2.0, 8.0])\n",
    "    \n",
    "    # Calculate cost\n",
    "    cost_huber = cost_function_huber(predicted_values, actual_values)\n",
    "    print(f\"Huber Loss: {cost_huber}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cd5ed00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Cross-Entropy Loss: 0.3375388286260043\n"
     ]
    }
   ],
   "source": [
    "## Code Snippet for cost function in Python -  Binary Cross-Entropy – Binary Classification\n",
    "import numpy as np\n",
    "def cost_function_binary_cross_entropy(predicted, actual):\n",
    "    \"\"\"\n",
    "    Calculate the binary cross-entropy loss between predicted and actual values.\n",
    "\n",
    "    Parameters:\n",
    "    predicted (np.ndarray): Predicted probabilities (between 0 and 1).\n",
    "    actual (np.ndarray): Actual binary labels (0 or 1).\n",
    "\n",
    "    Returns:\n",
    "    float: Binary cross-entropy loss.\n",
    "    \"\"\"\n",
    "    if len(predicted) != len(actual):\n",
    "        raise ValueError(\"Predicted and actual arrays must have the same length.\")\n",
    "    \n",
    "    # Clip predicted values to avoid log(0)\n",
    "    predicted = np.clip(predicted, 1e-15, 1 - 1e-15)\n",
    "    \n",
    "    bce = -np.mean(actual * np.log(predicted) + (1 - actual) * np.log(1 - predicted))\n",
    "    return bce\n",
    "\n",
    "# Example usage for Binary Cross-Entropy\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data\n",
    "    predicted_probabilities = np.array([0.9, 0.1, 0.8, 0.6])\n",
    "    actual_labels = np.array([1, 0, 1, 0])\n",
    "    \n",
    "    # Calculate cost\n",
    "    cost_bce = cost_function_binary_cross_entropy(predicted_probabilities, actual_labels)\n",
    "    print(f\"Binary Cross-Entropy Loss: {cost_bce}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc69168a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Cross-Entropy Loss: 0.5202159160882228\n"
     ]
    }
   ],
   "source": [
    "## code snippet for cost function in Python - Categorical Cross-Entropy – Multi-class Classification\n",
    "import numpy as np\n",
    "def cost_function_categorical_cross_entropy(predicted, actual):\n",
    "    \"\"\"\n",
    "    Calculate the categorical cross-entropy loss between predicted and actual values.\n",
    "\n",
    "    Parameters:\n",
    "    predicted (np.ndarray): Predicted probabilities for each class.\n",
    "    actual (np.ndarray): Actual class labels (one-hot encoded).\n",
    "\n",
    "    Returns:\n",
    "    float: Categorical cross-entropy loss.\n",
    "    \"\"\"\n",
    "    if len(predicted) != len(actual):\n",
    "        raise ValueError(\"Predicted and actual arrays must have the same length.\")\n",
    "    \n",
    "    # Clip predicted values to avoid log(0)\n",
    "    predicted = np.clip(predicted, 1e-15, 1 - 1e-15)\n",
    "    \n",
    "    cce = -np.mean(np.sum(actual * np.log(predicted), axis=1))\n",
    "    return cce\n",
    "\n",
    "# Example usage for Categorical Cross-Entropy\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data\n",
    "    predicted_probabilities = np.array([[0.7, 0.2, 0.1],\n",
    "                                         [0.1, 0.6, 0.3],\n",
    "                                         [0.2, 0.3, 0.5]])\n",
    "    actual_labels = np.array([[1, 0, 0],\n",
    "                              [0, 1, 0],\n",
    "                              [0, 0, 1]])\n",
    "    \n",
    "    # Calculate cost\n",
    "    cost_cce = cost_function_categorical_cross_entropy(predicted_probabilities, actual_labels)\n",
    "    print(f\"Categorical Cross-Entropy Loss: {cost_cce}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b065ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
