{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14abc2a0",
   "metadata": {},
   "source": [
    "# ðŸ”¹ AdaGrad (Adaptive Gradient Optimizer)\n",
    "\n",
    "## 1. Theory\n",
    "\n",
    "**AdaGrad (Adaptive Gradient)** is an optimizer that **adapts the learning rate** for each parameter individually based on how frequently it has been updated.  \n",
    "- Parameters with **frequent updates** â†’ get **smaller learning rates**.  \n",
    "- Parameters with **rare updates** â†’ retain **larger learning rates**.\n",
    "\n",
    "This makes AdaGrad particularly effective for **sparse data** (e.g., NLP, recommender systems).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Formula\n",
    "\n",
    "Weight update rule:\n",
    "\n",
    "$$\n",
    "g_t = \\frac{\\partial J}{\\partial w_t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "G_t = G_{t-1} + g_t^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{G_t} + \\epsilon} \\cdot g_t\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( g_t \\) â†’ gradient at time \\( t \\)\n",
    "- \\( G_t \\) â†’ sum of squares of past gradients (per parameter)\n",
    "- \\( \\eta \\) â†’ initial learning rate\n",
    "- \\( \\epsilon \\) â†’ small constant to prevent division by zero\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Intuition\n",
    "\n",
    "- AdaGrad maintains a separate **learning rate** for every parameter.  \n",
    "- Frequently updated parameters shrink in step size, preventing overshooting.  \n",
    "- Rarely updated parameters keep larger steps, helping in sparse features.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Advantages and Disadvantages\n",
    "\n",
    "| **Advantages**                                        | **Disadvantages**                                      |\n",
    "|------------------------------------------------------|------------------------------------------------------|\n",
    "| Works well for sparse data (e.g., NLP, text classification). | Learning rate keeps decreasing, may lead to early stopping. |\n",
    "| Automatically adjusts learning rates for each parameter.     | Cannot recover once learning rate becomes too small.  |\n",
    "| Requires less tuning of learning rate.                        | Not ideal for deep networks due to aggressive decay. |\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Use Cases\n",
    "\n",
    "- âœ… Natural Language Processing (NLP)  \n",
    "- âœ… Sparse feature problems (e.g., recommendation systems)  \n",
    "- âœ… Logistic Regression with sparse inputs  \n",
    "\n",
    "---\n",
    "\n",
    "## 6. Interview Questions and Answers\n",
    "\n",
    "### **Q1: Why is AdaGrad good for sparse data?**\n",
    "**Answer:**  \n",
    "- It maintains **larger learning rates** for infrequently updated parameters, ensuring they still learn effectively.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q2: What is the main limitation of AdaGrad?**\n",
    "**Answer:**  \n",
    "- The accumulated squared gradients \\( G_t \\) keep growing, causing the learning rate to shrink to near zero over time, which may stop learning prematurely.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Conclusion\n",
    "- **AdaGrad** is adaptive and great for sparse features.  \n",
    "- However, its **learning rate decay** is too aggressive, which led to the development of **RMSProp** and **Adam**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928aefb4",
   "metadata": {},
   "source": [
    "# ðŸ”¹ AdaDelta & RMSProp Optimizers\n",
    "\n",
    "---\n",
    "\n",
    "## 1. RMSProp (Root Mean Square Propagation)\n",
    "\n",
    "### âœ… Theory\n",
    "- **RMSProp** is an improvement over AdaGrad.\n",
    "- It solves AdaGradâ€™s problem of **aggressively decreasing learning rates** by using an **exponentially decaying average** of past squared gradients.\n",
    "- Keeps learning rate more stable throughout training.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Formula\n",
    "\n",
    "For parameter \\( w \\):\n",
    "\n",
    "$$\n",
    "E[g^2]_t = \\gamma E[g^2]_{t-1} + (1 - \\gamma) g_t^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{E[g^2]_t} + \\epsilon} g_t\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( \\gamma \\) â†’ decay rate (commonly 0.9)\n",
    "- \\( E[g^2]_t \\) â†’ moving average of squared gradients\n",
    "- \\( \\eta \\) â†’ learning rate\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Advantages and Disadvantages\n",
    "\n",
    "| **Advantages**                                       | **Disadvantages**                                      |\n",
    "|-----------------------------------------------------|------------------------------------------------------|\n",
    "| Solves AdaGradâ€™s learning rate decay problem.       | Requires tuning of decay rate \\( \\gamma \\).          |\n",
    "| Performs well on non-stationary problems (e.g., RNNs). | Can still oscillate without momentum.                |\n",
    "| Commonly used for training deep networks.           | Slightly more computationally intensive.             |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Use Cases\n",
    "- âœ… Recurrent Neural Networks (RNNs)  \n",
    "- âœ… Non-stationary data (changing patterns)  \n",
    "- âœ… Deep learning tasks where AdaGrad fails  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. AdaDelta\n",
    "\n",
    "### âœ… Theory\n",
    "- **AdaDelta** is an **extension of AdaGrad** that also fixes the **decaying learning rate problem**.\n",
    "- Unlike RMSProp, it **does not require a manually set learning rate \\( \\eta \\)**.  \n",
    "- Uses a **moving window of gradient updates** to adapt step sizes.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Formula\n",
    "\n",
    "1. Compute running average of squared gradients:\n",
    "   $$\n",
    "   E[g^2]_t = \\gamma E[g^2]_{t-1} + (1 - \\gamma) g_t^2\n",
    "   $$\n",
    "\n",
    "2. Compute parameter updates using running average of updates:\n",
    "   $$\n",
    "   \\Delta w_t = - \\frac{\\sqrt{E[\\Delta w^2]_{t-1} + \\epsilon}}{\\sqrt{E[g^2]_t + \\epsilon}} g_t\n",
    "   $$\n",
    "\n",
    "3. Update parameters:\n",
    "   $$\n",
    "   w_{t+1} = w_t + \\Delta w_t\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Advantages and Disadvantages\n",
    "\n",
    "| **Advantages**                                      | **Disadvantages**                                      |\n",
    "|-----------------------------------------------------|------------------------------------------------------|\n",
    "| Eliminates the need for a global learning rate \\( \\eta \\). | More complex than RMSProp.                           |\n",
    "| Prevents learning rate from shrinking too much.     | Slightly slower than Adam in practice.               |\n",
    "| Works well with sparse and dense data.              | Less commonly used compared to Adam.                 |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Use Cases\n",
    "- âœ… When you want **adaptive learning** without tuning learning rate.  \n",
    "- âœ… Suitable for large-scale deep learning problems.  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. Interview Q&A\n",
    "\n",
    "### **Q1: How is RMSProp different from AdaGrad?**\n",
    "**Answer:**  \n",
    "- RMSProp uses an **exponential moving average** of gradients instead of summing them, preventing the learning rate from decaying too quickly.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q2: Why is AdaDelta considered an improvement over AdaGrad and RMSProp?**\n",
    "**Answer:**  \n",
    "- AdaDelta **removes the need for a manually set learning rate** and maintains stable updates.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Conclusion\n",
    "- **RMSProp**: Fixes AdaGradâ€™s decay issue, widely used in RNNs.  \n",
    "- **AdaDelta**: Similar to RMSProp but eliminates the need for setting \\( \\eta \\).  \n",
    "- These optimizers influenced the development of **Adam**, which combines their strengths.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1913320",
   "metadata": {},
   "source": [
    "# ðŸ”¹ Adam Optimizer (Adaptive Moment Estimation)\n",
    "\n",
    "## 1. Theory\n",
    "\n",
    "**Adam (Adaptive Moment Estimation)** combines the advantages of:\n",
    "- âœ… **RMSProp** (adaptive learning rate per parameter)  \n",
    "- âœ… **Momentum** (uses exponentially decaying averages of past gradients)  \n",
    "\n",
    "Adam maintains two moving averages:\n",
    "- \\( m_t \\) â†’ first moment (mean of gradients, like momentum)\n",
    "- \\( v_t \\) â†’ second moment (uncentered variance, like RMSProp)\n",
    "\n",
    "This makes Adam:\n",
    "- **fast to converge**\n",
    "- **robust to noisy gradients**\n",
    "- **suitable for large datasets & deep networks**\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Formula\n",
    "\n",
    "1. Compute moving averages:\n",
    "   $$\n",
    "   m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n",
    "   $$\n",
    "   $$\n",
    "   v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\n",
    "   $$\n",
    "\n",
    "2. Bias correction:\n",
    "   $$\n",
    "   \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}\n",
    "   $$\n",
    "   $$\n",
    "   \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "   $$\n",
    "\n",
    "3. Parameter update:\n",
    "   $$\n",
    "   w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t\n",
    "   $$\n",
    "\n",
    "Where:\n",
    "- \\( \\beta_1 \\) â†’ decay rate for momentum (default 0.9)  \n",
    "- \\( \\beta_2 \\) â†’ decay rate for RMSProp term (default 0.999)  \n",
    "- \\( \\eta \\) â†’ learning rate (default 0.001)  \n",
    "- \\( \\epsilon \\) â†’ small constant to avoid division by zero  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. Intuition\n",
    "\n",
    "- Adam keeps track of **both**:\n",
    "  - **average gradient** (helps direction)\n",
    "  - **average squared gradient** (controls step size)\n",
    "- This allows **stable and fast** optimization.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Advantages and Disadvantages\n",
    "\n",
    "| **Advantages**                                              | **Disadvantages**                                      |\n",
    "|------------------------------------------------------------|------------------------------------------------------|\n",
    "| Combines benefits of Momentum & RMSProp.                   | May sometimes lead to worse generalization than SGD. |\n",
    "| Works well with noisy, sparse gradients.                   | Requires tuning of multiple hyperparameters.         |\n",
    "| Default optimizer in most deep learning frameworks.        | Can converge to sharp minima in some cases.          |\n",
    "| Adaptive learning rates for each parameter.                | Heavier computation than simple SGD.                 |\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Use Cases\n",
    "\n",
    "- âœ… Deep Neural Networks (CNNs, RNNs, Transformers)  \n",
    "- âœ… Large datasets with sparse gradients (e.g., NLP)  \n",
    "- âœ… Most modern architectures (default optimizer in TensorFlow & PyTorch)  \n",
    "\n",
    "---\n",
    "\n",
    "## 6. Interview Questions and Answers\n",
    "\n",
    "### **Q1: How is Adam better than SGD with Momentum?**\n",
    "**Answer:**  \n",
    "- Adam adapts the learning rate for each parameter individually using the second moment estimate, making it more efficient and stable.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q2: Why does Adam use bias correction?**\n",
    "**Answer:**  \n",
    "- At the start of training, \\( m_t \\) and \\( v_t \\) are biased toward zero. Bias correction ensures proper scaling.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Conclusion\n",
    "- **Adam** is the most popular optimizer for deep learning due to its **adaptive learning rate** and **fast convergence**.  \n",
    "- However, for some problems, **SGD with Momentum** may provide better generalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12cf0ca",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
