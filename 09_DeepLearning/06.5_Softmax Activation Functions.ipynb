{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd016eb7",
   "metadata": {},
   "source": [
    "# ðŸ”¹ Softmax Activation Function\n",
    "\n",
    "## 1. Theory\n",
    "\n",
    "The **Softmax Activation Function** is widely used in the **output layer** of neural networks for **multi-class classification** problems.  \n",
    "It converts raw scores (logits) into a **probability distribution**, where:\n",
    "\n",
    "- Each output value lies in the range (0,1).\n",
    "- The sum of all outputs equals 1.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mathematical Formula**\n",
    "\n",
    "For a vector \\( z = [z_1, z_2, ..., z_k] \\) representing the raw scores for \\( k \\) classes:\n",
    "\n",
    "$$\n",
    "\\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{k} e^{z_j}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( z_i \\) = input score for class \\( i \\)\n",
    "- \\( \\sigma(z_i) \\) = probability of class \\( i \\)\n",
    "- \\( k \\) = number of classes\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Properties\n",
    "\n",
    "- Outputs are **positive** and **sum to 1**.\n",
    "- The largest logit gets the highest probability.\n",
    "- Sensitive to the **relative differences** between logits.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Advantages and Disadvantages\n",
    "\n",
    "| **Aspect**          | **Advantages**                                                        | **Disadvantages**                                                |\n",
    "|---------------------|------------------------------------------------------------------------|------------------------------------------------------------------|\n",
    "| **Probability Output** | Converts logits into probabilities, making interpretation easy.    | Can lead to **overconfidence** when logits have large magnitude. |\n",
    "| **Multi-class Use** | Ideal for multi-class classification tasks.                           | Sensitive to **outliers** and can cause **vanishing gradients** if inputs are large. |\n",
    "| **Differentiability** | Fully differentiable, enabling gradient-based optimization.         | Computationally expensive for very large number of classes.     |\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Use Cases\n",
    "\n",
    "- **Output layer** for multi-class classifiers (e.g., image classification with CNNs).\n",
    "- Models like **Logistic Regression (multi-class)**, **Neural Networks**, and **Transformer architectures**.\n",
    "- In **Attention Mechanisms**, where softmax is used to compute attention weights.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Relation with Cross-Entropy Loss\n",
    "\n",
    "Softmax is commonly paired with the **Cross-Entropy Loss**:\n",
    "\n",
    "$$\n",
    "L = - \\sum_{i=1}^{k} y_i \\log(\\sigma(z_i))\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( y_i \\) is the true label (one-hot encoded)\n",
    "- \\( \\sigma(z_i) \\) is the predicted probability\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Interview Questions and Answers\n",
    "\n",
    "### **Q1: Why is Softmax used in multi-class classification?**\n",
    "**Answer:**  \n",
    "- Softmax outputs probabilities for each class, ensuring they sum to 1.  \n",
    "- This makes it suitable for selecting the class with the highest probability.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q2: Can Softmax be used in hidden layers?**\n",
    "**Answer:**  \n",
    "- No, it is typically used only in the **output layer** because it forces outputs to be a probability distribution.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q3: What is the difference between Sigmoid and Softmax?**\n",
    "**Answer:**  \n",
    "- **Sigmoid** is used for binary classification, outputting a single probability.  \n",
    "- **Softmax** generalizes sigmoid for multi-class classification, outputting probabilities for all classes.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Conclusion\n",
    "The **Softmax Activation Function** is essential for **multi-class classification** tasks, providing a normalized probability distribution.  \n",
    "It is almost always used with **Cross-Entropy Loss** for training classification networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2704e6ae",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
