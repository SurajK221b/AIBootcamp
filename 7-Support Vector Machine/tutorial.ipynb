{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fb60d06",
   "metadata": {},
   "source": [
    "## üìä Support Vector Machines (SVM) ‚Äì End-to-End Summary\n",
    "\n",
    "1. üìå Definition\n",
    "Support Vector Machines (SVM) are supervised learning algorithms used for classification, regression, and outlier detection. At its core, SVM aims to find the optimal hyperplane that maximally separates classes in a high-dimensional space.\n",
    "\n",
    "2. üß† Core Concept\n",
    "SVM operates under the following premise:\n",
    "\n",
    "Given a labeled dataset, the algorithm identifies the decision boundary (hyperplane) that maximizes the margin between different class labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf6abc",
   "metadata": {},
   "source": [
    "| **Aspect**                 | **Details**                                                                                                                                                                                                                   |\n",
    "| -------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Type**                   | Supervised Learning                                                                                                                                                                                                           |\n",
    "| **Applicable Tasks**       | Classification, Regression (SVR), Outlier Detection                                                                                                                                                                           |\n",
    "| **Objective**              | Maximize the margin between classes by identifying the optimal separating hyperplane                                                                                                                                          |\n",
    "| **Core Principle**         | Uses support vectors (critical boundary points) to define the margin                                                                                                                                                          |\n",
    "| **Mathematical Goal**      | Minimize:  $\\frac{1}{2} \\|w\\|^2$ <br> Subject to:  $y_i(w^T x_i + b) \\geq 1$                                                                                                                                                  |\n",
    "| **Linearly Separable?**    | Yes ‚Äì Uses hard margin <br> No ‚Äì Uses soft margin + kernel trick                                                                                                                                                              |\n",
    "| **Kernels Available**      | - Linear<br> - Polynomial<br> - RBF (Gaussian)<br> - Sigmoid                                                                                                                                                                  |\n",
    "| **Key Hyperparameters**    | - `C`: Regularization (controls trade-off between margin and classification error) <br> - `kernel`: Type of kernel <br> - `gamma`: Influence of a single data point (used in RBF/poly) <br> - `degree`: For polynomial kernel |\n",
    "| **Model Interpretability** | Low ‚Äì black-box model                                                                                                                                                                                                         |\n",
    "| **Scalability**            | Poor with large datasets; computationally expensive (training complexity: \\~O(n¬≥))                                                                                                                                            |\n",
    "| **Preprocessing Needs**    | - Feature scaling (e.g., StandardScaler) <br> - Remove irrelevant features                                                                                                                                                    |\n",
    "| **Strengths**              | - Effective in high-dimensional spaces <br> - Memory efficient <br> - Robust to overfitting                                                                                                                                   |\n",
    "| **Limitations**            | - Slow on large datasets <br> - Sensitive to outliers <br> - Less interpretable                                                                                                                                               |\n",
    "| **Best For**               | - Text classification <br> - Image classification <br> - Medical diagnostics                                                                                                                                                  |\n",
    "| **Python Library**         | `sklearn.svm.SVC` (for classification) <br> `sklearn.svm.SVR` (for regression)                                                                                                                                                |\n",
    "| **Evaluation Metrics**     | Classification: Accuracy, Precision, Recall, F1-score <br> Regression (SVR): MAE, MSE, R¬≤                                                                                                                                     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d432d73",
   "metadata": {},
   "source": [
    "### üåê Real-World Use Cases\n",
    "| **Industry**  | **Application**                           |\n",
    "| ------------- | ----------------------------------------- |\n",
    "| Finance       | Fraud detection, credit scoring           |\n",
    "| Healthcare    | Cancer classification, disease prediction |\n",
    "| Retail        | Customer segmentation                     |\n",
    "| Cybersecurity | Intrusion and malware detection           |\n",
    "| Manufacturing | Predictive maintenance, defect detection  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ec8cdd",
   "metadata": {},
   "source": [
    "### 1. üîç Foundational Concept\n",
    "Support Vector Machines (SVM) are based on the principle of structural risk minimization rather than empirical risk minimization. Instead of merely minimizing classification errors on the training set, SVM aims to maximize the margin between classes, thereby enhancing the model's ability to generalize on unseen data.\n",
    "\n",
    "Margin is defined as the distance between the separating hyperplane and the closest data points (support vectors).\n",
    "\n",
    "### 2. üß± Linear SVM\n",
    "In a binary classification scenario with linearly separable classes, SVM tries to solve the following optimization problem:\n",
    "\n",
    "### 3. üßÆ Soft Margin SVM (for Non-Separable Data)\n",
    "When data is not linearly separable, SVM introduces slack variables \n",
    "ùúâ\n",
    "ùëñ\n",
    "Œæ \n",
    "i\n",
    "‚Äã\n",
    "  and modifies the objective function to allow for some misclassifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e23633",
   "metadata": {},
   "source": [
    "### 4. üåÄ Kernel Trick (Non-Linear SVM)\n",
    "If data is not linearly separable in the original space, SVM uses the kernel trick to project data into a higher-dimensional space where it becomes linearly separable.\n",
    "| Kernel         | Mathematical Form                       | Use Case                                           |\n",
    "| -------------- | --------------------------------------- | -------------------------------------------------- |\n",
    "| Linear         | $K(x, x') = x^T x'$                     | Linearly separable data                            |\n",
    "| Polynomial     | $K(x, x') = (x^T x' + r)^d$             | Data with polynomial decision boundaries           |\n",
    "| RBF (Gaussian) | $K(x, x') = \\exp(-\\gamma \\|x - x'\\|^2)$ | Most widely used, handles non-linear problems well |\n",
    "| Sigmoid        | $K(x, x') = \\tanh(\\alpha x^T x' + r)$   | Similar to neural networks (less common)           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc6858d",
   "metadata": {},
   "source": [
    "### ‚úÖ Basic SVC Implementation Using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "149af4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9666666666666667\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      0.90      0.95        10\n",
      "           2       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 2. Load dataset\n",
    "data = load_iris()\n",
    "X = data.data        # Feature matrix\n",
    "y = data.target      # Target vector\n",
    "\n",
    "# (Optional) For binary classification: restrict to two classes\n",
    "# X = X[y != 2]\n",
    "# y = y[y != 2]\n",
    "\n",
    "# 3. Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 4. Feature scaling (very important for SVM)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 5. Train the SVM classifier\n",
    "model = SVC(kernel='rbf', C=1.0, gamma='scale')  # Default kernel is 'rbf'\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# 7. Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48287029",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Key Parameters of SVC\n",
    "| Parameter | Description                                                                                       |\n",
    "| --------- | ------------------------------------------------------------------------------------------------- |\n",
    "| `kernel`  | Specifies the kernel type (`'linear'`, `'rbf'`, `'poly'`, `'sigmoid'`)                            |\n",
    "| `C`       | Regularization parameter (trade-off between margin width and classification error)                |\n",
    "| `gamma`   | Kernel coefficient for `'rbf'`, `'poly'`, and `'sigmoid'`. Controls decision boundary complexity. |\n",
    "| `degree`  | Degree of the polynomial kernel (if `kernel='poly'`)                                              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7eb52a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
