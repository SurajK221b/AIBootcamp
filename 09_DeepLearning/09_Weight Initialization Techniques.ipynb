{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e367add",
   "metadata": {},
   "source": [
    "# ðŸ”¹ Weight Initialization Techniques\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Proper **weight initialization** is crucial in deep learning because:\n",
    "- âœ… Prevents **vanishing/exploding gradients**\n",
    "- âœ… Speeds up convergence\n",
    "- âœ… Improves model performance\n",
    "\n",
    "Poor initialization can cause:\n",
    "- **Slow learning**\n",
    "- **Unstable gradients**\n",
    "- **Difficulty in training deep networks**\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Common Weight Initialization Methods\n",
    "\n",
    "| **Technique**               | **Formula**                                                                 | **Suitable For**                                | **Remarks**                                       |\n",
    "|-----------------------------|-----------------------------------------------------------------------------|------------------------------------------------|--------------------------------------------------|\n",
    "| **Zero Initialization**     | $$ w = 0 $$                                                                 | None (only for biases)                         | All neurons learn the same features â†’ poor training. |\n",
    "| **Random Initialization**   | $$ w \\sim U(-a, a) $$                                                       | Early simple networks                           | Breaks symmetry but may cause gradient issues.    |\n",
    "| **Xavier (Glorot) Initialization** | $$ w \\sim U\\Big[-\\sqrt{\\frac{6}{n_{in} + n_{out}}}, \\sqrt{\\frac{6}{n_{in} + n_{out}}}\\Big] $$ | Sigmoid, Tanh activations                      | Maintains variance across layers.                 |\n",
    "| **He Initialization**       | $$ w \\sim N\\Big(0, \\sqrt{\\frac{2}{n_{in}}}\\Big) $$                         | ReLU and variants (Leaky ReLU, PReLU)          | Prevents vanishing gradients in deep ReLU nets.   |\n",
    "| **LeCun Initialization**    | $$ w \\sim N\\Big(0, \\sqrt{\\frac{1}{n_{in}}}\\Big) $$                         | SELU activation                                | Optimized for self-normalizing networks.          |\n",
    "| **Orthogonal Initialization** | Generate a random orthogonal matrix for weights.                         | RNNs, deep linear networks                     | Preserves norm; stable training.                  |\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Formulas Explained\n",
    "\n",
    "- \\( n_{in} \\) â†’ number of input neurons to the layer  \n",
    "- \\( n_{out} \\) â†’ number of output neurons  \n",
    "- Proper scaling ensures signal variance remains constant across layers.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Best Practices\n",
    "\n",
    "- âœ… Use **Xavier** for Sigmoid/Tanh networks.  \n",
    "- âœ… Use **He Initialization** for ReLU-based networks.  \n",
    "- âœ… Use **LeCun Initialization** for SELU activations.  \n",
    "- âœ… Avoid initializing all weights to the same value.  \n",
    "\n",
    "---\n",
    "\n",
    "## 5. Interview Questions and Answers\n",
    "\n",
    "### **Q1: Why not initialize all weights to zero?**\n",
    "**Answer:**  \n",
    "- All neurons would receive identical gradients and learn the same features, preventing effective learning.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q2: Why is He Initialization preferred for ReLU?**\n",
    "**Answer:**  \n",
    "- ReLU activates only half of the neurons (positive region). He initialization compensates by scaling variance to avoid vanishing gradients.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q3: What happens if weights are too large?**\n",
    "**Answer:**  \n",
    "- Can lead to **exploding gradients**, unstable training, and divergence.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Conclusion\n",
    "- Weight initialization is critical to stable and efficient training.  \n",
    "- Choosing the correct initialization method depends on the **activation function** and **network architecture**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c324bca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b953ef8",
   "metadata": {},
   "source": [
    "# ðŸ”¹ Weight Initialization Using Normal & Uniform Distributions\n",
    "\n",
    "## 1. Normal Distribution Initialization\n",
    "\n",
    "When weights are initialized using a **Normal (Gaussian)** distribution:\n",
    "\n",
    "$$\n",
    "w \\sim N(\\mu, \\sigma^2)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( \\mu \\) â†’ mean (commonly 0)\n",
    "- \\( \\sigma^2 \\) â†’ variance (controls spread)\n",
    "- \\( w \\) â†’ weight parameter\n",
    "\n",
    "Examples:\n",
    "- **Xavier Normal Initialization**:  \n",
    "  $$\n",
    "  w \\sim N\\Big(0, \\frac{2}{n_{in} + n_{out}}\\Big)\n",
    "  $$\n",
    "\n",
    "- **He Normal Initialization**:  \n",
    "  $$\n",
    "  w \\sim N\\Big(0, \\frac{2}{n_{in}}\\Big)\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Uniform Distribution Initialization\n",
    "\n",
    "When weights are initialized using a **Uniform** distribution:\n",
    "\n",
    "$$\n",
    "w \\sim U(-a, a)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( a \\) â†’ limit that defines the range\n",
    "- \\( w \\) â†’ weight parameter\n",
    "\n",
    "Examples:\n",
    "- **Xavier Uniform Initialization**:  \n",
    "  $$\n",
    "  w \\sim U\\Big(-\\sqrt{\\frac{6}{n_{in} + n_{out}}}, \\sqrt{\\frac{6}{n_{in} + n_{out}}}\\Big)\n",
    "  $$\n",
    "\n",
    "- **He Uniform Initialization**:  \n",
    "  $$\n",
    "  w \\sim U\\Big(-\\sqrt{\\frac{6}{n_{in}}}, \\sqrt{\\frac{6}{n_{in}}}\\Big)\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Conclusion\n",
    "- **Normal Initialization** â†’ draws weights from a Gaussian distribution.  \n",
    "- **Uniform Initialization** â†’ draws weights from a uniform range.  \n",
    "- Proper scaling (using \\( n_{in} \\) and \\( n_{out} \\)) prevents vanishing or exploding gradients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b2be4e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
