{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04cd6f4c",
   "metadata": {},
   "source": [
    "# ðŸ”¹ Gradient Descent Optimizer\n",
    "\n",
    "## 1. Theory\n",
    "\n",
    "**Gradient Descent** is the most fundamental optimization algorithm used in training neural networks.  \n",
    "It updates model parameters (weights) in the direction **opposite to the gradient** of the cost function to minimize the loss.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Formula\n",
    "\n",
    "For weight \\( w \\) at iteration \\( t \\):\n",
    "$$\n",
    "w^{(t+1)} = w^{(t)} - \\eta \\frac{\\partial J}{\\partial w}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( w \\) â†’ weight\n",
    "- \\( \\eta \\) â†’ learning rate (controls step size)\n",
    "- \\( \\frac{\\partial J}{\\partial w} \\) â†’ gradient of cost function \\( J \\) w.r.t \\( w \\)\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Types of Gradient Descent\n",
    "\n",
    "| **Type**                        | **Description**                                                         | **Use Case**                               |\n",
    "|---------------------------------|------------------------------------------------------------------------|-------------------------------------------|\n",
    "| **Batch Gradient Descent**      | Uses the **entire dataset** to compute gradients per iteration.       | Small datasets where computation is feasible. |\n",
    "| **Stochastic Gradient Descent (SGD)** | Uses **one sample** at a time for weight updates.                    | Large datasets, online learning.          |\n",
    "| **Mini-Batch Gradient Descent** | Uses a **small batch** of samples per update.                         | Most widely used in deep learning.        |\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Advantages and Disadvantages\n",
    "\n",
    "| **Advantages**                                        | **Disadvantages**                                      |\n",
    "|------------------------------------------------------|------------------------------------------------------|\n",
    "| Simple to understand and implement.                  | Sensitive to choice of learning rate \\( \\eta \\).     |\n",
    "| Guarantees convergence for convex functions.         | Can get stuck in local minima (for non-convex problems). |\n",
    "| Forms the basis for all advanced optimizers.         | Slow convergence for deep networks.                  |\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Interview Questions and Answers\n",
    "\n",
    "### **Q1: Why is learning rate important in Gradient Descent?**\n",
    "**Answer:**  \n",
    "- A very high \\( \\eta \\) â†’ may overshoot minima.  \n",
    "- A very low \\( \\eta \\) â†’ very slow convergence.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Q2: Why is Mini-Batch Gradient Descent preferred in deep learning?**\n",
    "**Answer:**  \n",
    "- It balances **computational efficiency** and **convergence stability**, making it the standard choice.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Conclusion\n",
    "Gradient Descent is the **foundation** of optimization in machine learning.  \n",
    "All advanced optimizers (e.g., SGD with Momentum, RMSProp, Adam) are **extensions** of Gradient Descent to improve convergence speed and stability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb62344",
   "metadata": {},
   "source": [
    "# ðŸ”¹ Stochastic Gradient Descent (SGD)\n",
    "\n",
    "## 1. Theory\n",
    "\n",
    "**Stochastic Gradient Descent (SGD)** is a variant of Gradient Descent where the model parameters are updated **for each training example** rather than using the entire dataset.  \n",
    "This introduces **stochasticity** (randomness), which can help escape local minima but also causes noisy updates.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Formula\n",
    "\n",
    "For weight \\( w \\) at iteration \\( t \\):\n",
    "$$\n",
    "w^{(t+1)} = w^{(t)} - \\eta \\frac{\\partial J(w; x^{(i)}, y^{(i)})}{\\partial w}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( (x^{(i)}, y^{(i)}) \\) â†’ single training example\n",
    "- \\( \\eta \\) â†’ learning rate\n",
    "- \\( \\frac{\\partial J}{\\partial w} \\) â†’ gradient of loss for that single example\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Workflow\n",
    "\n",
    "1. Shuffle dataset.\n",
    "2. For each sample \\( i \\), compute gradient \\( \\nabla J(w; x^{(i)}, y^{(i)}) \\).\n",
    "3. Update weights using the formula.\n",
    "4. Repeat for multiple epochs.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Advantages and Disadvantages\n",
    "\n",
    "| **Advantages**                                              | **Disadvantages**                                      |\n",
    "|------------------------------------------------------------|------------------------------------------------------|\n",
    "| Faster updates since only one sample is processed at a time.| Updates are noisy; loss fluctuates.                   |\n",
    "| Helps escape local minima due to randomness.               | May have difficulty converging to the exact minimum.  |\n",
    "| Suitable for large datasets (online learning).             | Requires careful tuning of learning rate.             |\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Improvements Over SGD\n",
    "\n",
    "- âœ… **Mini-Batch SGD** â†’ Uses a batch of data to reduce noise while maintaining efficiency.  \n",
    "- âœ… **SGD with Momentum** â†’ Adds a momentum term to smooth updates and accelerate convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Interview Questions and Answers\n",
    "\n",
    "### **Q1: Why is SGD faster than Batch Gradient Descent?**\n",
    "**Answer:**  \n",
    "- Because it updates parameters after processing each sample instead of waiting for the entire dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q2: Why does SGD sometimes fail to converge?**\n",
    "**Answer:**  \n",
    "- The updates are noisy; without a decaying learning rate, it may oscillate around the minimum.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Conclusion\n",
    "- **SGD** is widely used for training large-scale machine learning models.  \n",
    "- In practice, **Mini-Batch SGD** with optimizers like **Momentum** or **Adam** is preferred for deep learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b9841d",
   "metadata": {},
   "source": [
    "# ðŸ”¹ Mini-Batch Stochastic Gradient Descent (Mini-Batch SGD)\n",
    "\n",
    "## 1. Theory\n",
    "\n",
    "**Mini-Batch SGD** is an improvement over both **Batch Gradient Descent** and **Stochastic Gradient Descent (SGD)**.  \n",
    "- Instead of updating weights for each sample (SGD) or using the entire dataset (Batch GD),  \n",
    "- **Mini-Batch SGD** uses a **small batch** of data (e.g., 32, 64, 128 samples) to compute the gradient.\n",
    "\n",
    "This combines the **stability** of Batch Gradient Descent with the **efficiency** of SGD.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Formula\n",
    "\n",
    "For weight \\( w \\) at iteration \\( t \\):\n",
    "$$\n",
    "w^{(t+1)} = w^{(t)} - \\eta \\frac{1}{B} \\sum_{i=1}^{B} \\frac{\\partial J(w; x^{(i)}, y^{(i)})}{\\partial w}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( B \\) â†’ batch size (number of samples in a mini-batch)\n",
    "- \\( (x^{(i)}, y^{(i)}) \\) â†’ training samples in the batch\n",
    "- \\( \\eta \\) â†’ learning rate\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Workflow\n",
    "\n",
    "1. Divide dataset into mini-batches.\n",
    "2. For each batch:\n",
    "   - Compute average gradient over the batch.\n",
    "   - Update weights using the computed gradient.\n",
    "3. Repeat for multiple epochs.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Advantages and Disadvantages\n",
    "\n",
    "| **Advantages**                                                   | **Disadvantages**                                      |\n",
    "|------------------------------------------------------------------|------------------------------------------------------|\n",
    "| Reduces noise compared to SGD.                                   | Slightly more complex than plain SGD.                |\n",
    "| Faster training compared to Batch Gradient Descent.             | Still may oscillate if learning rate is not tuned.   |\n",
    "| Efficiently utilizes **vectorized operations (GPU-friendly)**.  | Choice of batch size affects performance.            |\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Why Mini-Batch is Preferred in Deep Learning?\n",
    "\n",
    "- âœ… Provides **better generalization** than Batch GD.  \n",
    "- âœ… **Faster convergence** than SGD due to reduced noise.  \n",
    "- âœ… Allows **parallel computation** using GPUs.  \n",
    "- âœ… Standard practice in deep learning frameworks (TensorFlow, PyTorch).\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Interview Questions and Answers\n",
    "\n",
    "### **Q1: What is the typical size of a mini-batch?**\n",
    "**Answer:**  \n",
    "- Common sizes are **32, 64, 128**. It depends on hardware and dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q2: Why is Mini-Batch SGD better than Batch GD or SGD?**\n",
    "**Answer:**  \n",
    "- It combines the **computational efficiency** of batch processing and the **regularization effect** (noise) of SGD, leading to faster and stable convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Conclusion\n",
    "- **Mini-Batch SGD** is the **default optimizer** used in most deep learning models.  \n",
    "- It serves as the **foundation** for advanced optimizers like **Adam** and **RMSProp**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd06f4ae",
   "metadata": {},
   "source": [
    "# ðŸ”¹ SGD with Momentum\n",
    "\n",
    "## 1. Theory\n",
    "\n",
    "**Stochastic Gradient Descent with Momentum** is an enhanced version of SGD.  \n",
    "It introduces a **momentum term** that helps the optimizer:\n",
    "\n",
    "- Accelerate in the direction of consistent gradients.  \n",
    "- Reduce oscillations, especially in areas with steep and narrow curves.  \n",
    "\n",
    "This idea is inspired by **physics**: the optimizer behaves like a ball rolling down a hill, gaining speed in the right direction.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Formula\n",
    "\n",
    "SGD with Momentum updates weights using both the current gradient and the past update:\n",
    "\n",
    "$$\n",
    "v_t = \\beta v_{t-1} + \\eta \\frac{\\partial J}{\\partial w}\n",
    "$$\n",
    "\n",
    "$$\n",
    "w^{(t+1)} = w^{(t)} - v_t\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( v_t \\) â†’ velocity term (accumulated gradient)\n",
    "- \\( \\beta \\) â†’ momentum coefficient (commonly 0.9)\n",
    "- \\( \\eta \\) â†’ learning rate\n",
    "- \\( \\frac{\\partial J}{\\partial w} \\) â†’ gradient of cost function\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Intuition\n",
    "\n",
    "- **Plain SGD** â†’ can oscillate back and forth in steep valleys.  \n",
    "- **SGD with Momentum** â†’ accumulates gradients, allowing **faster convergence** and **less oscillation**.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Advantages and Disadvantages\n",
    "\n",
    "| **Advantages**                                              | **Disadvantages**                                      |\n",
    "|------------------------------------------------------------|------------------------------------------------------|\n",
    "| Accelerates learning in the correct direction.             | Requires tuning of momentum parameter \\( \\beta \\).   |\n",
    "| Reduces oscillations in narrow ravines.                    | May overshoot if \\( \\beta \\) is too high.            |\n",
    "| Converges faster than plain SGD.                           | Slightly more computationally expensive.             |\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Interview Questions and Answers\n",
    "\n",
    "### **Q1: What is the role of the momentum term \\( \\beta \\)?**\n",
    "**Answer:**  \n",
    "- \\( \\beta \\) controls how much of the past gradients influence the current update.  \n",
    "- Common value: **0.9** (meaning 90% of previous gradient direction is retained).\n",
    "\n",
    "---\n",
    "\n",
    "### **Q2: How does momentum help in optimization?**\n",
    "**Answer:**  \n",
    "- It smooths out the updates, avoids oscillation, and helps escape shallow local minima faster.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Conclusion\n",
    "- **SGD with Momentum** is a powerful improvement over SGD.  \n",
    "- It forms the basis for **Nesterov Accelerated Gradient (NAG)** and other advanced optimizers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0609a9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
