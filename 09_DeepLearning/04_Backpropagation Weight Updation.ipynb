{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef04a3b",
   "metadata": {},
   "source": [
    "## 1. Backpropagation: Intuition\n",
    "Backpropagation (Backward Propagation of Errors) is the learning algorithm used by ANNs to minimize prediction error by adjusting weights.\n",
    "\n",
    "It works by propagating the error from the output layer back to the input layer.\n",
    "\n",
    "The objective is to find optimal weights that minimize the loss (error) function.\n",
    "\n",
    "Achieved using Gradient Descent optimization.\n",
    "\n",
    "## 2. How Backpropagation Works?\n",
    "The process occurs in four key steps:\n",
    "| **Step**             | **Process**                                                               | **Explanation**                                                 |\n",
    "| -------------------- | ------------------------------------------------------------------------- | --------------------------------------------------------------- |\n",
    "| **1. Forward Pass**  | Calculate predicted output $y'$.                                          | Input data is passed through the network to get predictions.    |\n",
    "| **2. Compute Loss**  | Evaluate error $E$ using a loss function (e.g., MSE, Cross-Entropy).      | Measures how far predictions are from actual values.            |\n",
    "| **3. Backward Pass** | Compute gradients ($\\frac{\\partial E}{\\partial w}$) using **Chain Rule**. | Gradients tell how much each weight contributes to error.       |\n",
    "| **4. Weight Update** | Update weights to minimize loss.                                          | Weights are adjusted in the opposite direction of the gradient. |\n",
    "\n",
    "## 3. Mathematical Representation\n",
    "Weight Update Rule:\n",
    "𝑤\n",
    "𝑛\n",
    "𝑒\n",
    "𝑤\n",
    "=\n",
    "𝑤\n",
    "𝑜\n",
    "𝑙\n",
    "𝑑\n",
    "−\n",
    "𝜂\n",
    "∂\n",
    "𝐸\n",
    "∂\n",
    "𝑤\n",
    "w \n",
    "new\n",
    "​\n",
    " =w \n",
    "old\n",
    "​\n",
    " −η \n",
    "∂w\n",
    "∂E\n",
    "​\n",
    " \n",
    "Where:\n",
    "\n",
    "𝑤\n",
    "w → Weight\n",
    "\n",
    "𝜂\n",
    "η → Learning Rate (controls step size)\n",
    "\n",
    "∂\n",
    "𝐸\n",
    "∂\n",
    "𝑤\n",
    "∂w\n",
    "∂E\n",
    "​\n",
    "  → Gradient of Loss \n",
    "𝐸\n",
    "E w.r.t weight \n",
    "𝑤\n",
    "w\n",
    "\n",
    "## 4. Role of Gradient Descent\n",
    "Gradient: Measures slope of error with respect to weights.\n",
    "\n",
    "Update Direction: Move weights opposite to the gradient to reduce error.\n",
    "\n",
    "Learning Rate (\n",
    "𝜂\n",
    "η):\n",
    "\n",
    "Too high → may overshoot minima.\n",
    "\n",
    "Too low → slow convergence.\n",
    "\n",
    "## 5. Intuitive Example\n",
    "Imagine you are standing on a hill (error surface) and want to reach the lowest valley (minimum error):\n",
    "\n",
    "The slope (gradient) tells you which way to step.\n",
    "\n",
    "You take small controlled steps (learning rate) to reach the bottom efficiently.\n",
    "\n",
    "## 6. Summary of Backpropagation Flow\n",
    "1. Forward Pass → Compute Output\n",
    "2. Compare with Actual → Calculate Error\n",
    "3. Backpropagate → Compute Gradients Layer by Layer\n",
    "4. Update Weights → Using Gradient Descent\n",
    "5. Repeat Until Error Minimizes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb60736f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
