{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c59e190a",
   "metadata": {},
   "source": [
    "# üîπ Tanh (Hyperbolic Tangent) Activation Function\n",
    "\n",
    "## 1. Theory\n",
    "\n",
    "The **Tanh Activation Function** (Hyperbolic Tangent) is a non-linear activation that maps input values into the range **(-1, 1)**.  \n",
    "It is essentially a scaled version of the sigmoid function, providing outputs centered around zero.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mathematical Formula**\n",
    "\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\n",
    "$$\n",
    "\n",
    "- For large positive \\( x \\), \\( \\tanh(x) \\to 1 \\)\n",
    "- For large negative \\( x \\), \\( \\tanh(x) \\to -1 \\)\n",
    "- For \\( x = 0 \\), \\( \\tanh(0) = 0 \\)\n",
    "\n",
    "---\n",
    "\n",
    "### **Derivative**\n",
    "\n",
    "The derivative of \\( \\tanh(x) \\) is:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx}\\tanh(x) = 1 - \\tanh^2(x)\n",
    "$$\n",
    "\n",
    "- The derivative is highest at \\( x = 0 \\) (equals 1)\n",
    "- For large \\( |x| \\), the derivative approaches 0, leading to **vanishing gradients**\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "## 3. Advantages and Disadvantages\n",
    "\n",
    "| **Aspect**          | **Advantages**                                                   | **Disadvantages**                                               |\n",
    "|---------------------|------------------------------------------------------------------|----------------------------------------------------------------|\n",
    "| **Range**           | Outputs in (-1,1), making it zero-centered and improving learning. | Still suffers from vanishing gradients for large \\( |x| \\).   |\n",
    "| **Smoothness**      | Differentiable everywhere, supports gradient-based learning.     | Slightly more computationally expensive than ReLU.            |\n",
    "| **Non-linearity**   | Allows networks to learn complex patterns.                       | Training may still be slower than with ReLU.                  |\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Use Cases\n",
    "\n",
    "- Used in **hidden layers** of neural networks (before ReLU became popular).\n",
    "- Frequently applied in **RNNs (Recurrent Neural Networks)** for internal state representation.\n",
    "- Used in **autoencoders** and **MLPs** when zero-centered activation is beneficial.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Sigmoid vs Tanh vs ReLU (Quick Comparison)\n",
    "\n",
    "| **Feature**          | **Sigmoid**         | **Tanh**            | **ReLU**               |\n",
    "|----------------------|----------------------|----------------------|------------------------|\n",
    "| **Range**            | (0,1)               | (-1,1)              | (0, ‚àû)                 |\n",
    "| **Zero-Centered?**   | ‚ùå No                | ‚úÖ Yes               | ‚úÖ Yes (for positive inputs) |\n",
    "| **Vanishing Gradient?**| ‚úÖ Yes             | ‚úÖ Yes               | ‚ùå Less likely         |\n",
    "| **Computation**      | Expensive (exp)     | Expensive (exp)     | Cheap                  |\n",
    "| **Preferred Use**    | Output (binary prob)| Hidden layers (older)| Hidden layers (modern) |\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Interview Questions and Answers\n",
    "\n",
    "### **Q1: Why is Tanh preferred over Sigmoid in hidden layers?**\n",
    "**Answer:**  \n",
    "- Tanh outputs values in **(-1,1)**, which is **zero-centered**.  \n",
    "- This reduces bias in gradient updates, leading to **faster convergence** than sigmoid.  \n",
    "- Sigmoid outputs (0,1), causing non-zero mean activations and slower learning.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q2: Does Tanh solve the Vanishing Gradient Problem?**\n",
    "**Answer:**  \n",
    "- No, it still suffers from vanishing gradients because its derivative approaches 0 for large \\( |x| \\).  \n",
    "- However, it is **less prone** than sigmoid due to its zero-centered nature.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q3: In which modern networks is Tanh still used?**\n",
    "**Answer:**  \n",
    "- Tanh is still widely used in **RNNs (e.g., LSTM, GRU)**, where bounded activations help control exploding values.  \n",
    "- It is less common in feedforward deep networks, where **ReLU** dominates.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Conclusion\n",
    "The **Tanh** activation is an improvement over sigmoid for hidden layers because it is **zero-centered**, enabling better gradient flow.  \n",
    "However, it is still susceptible to **vanishing gradients**, so modern architectures often use **ReLU** and its variants.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2403235",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
