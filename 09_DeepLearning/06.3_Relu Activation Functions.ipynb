{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65026300",
   "metadata": {},
   "source": [
    "# ðŸ”¹ ReLU, Leaky ReLU, and Parametric ReLU Activation Functions\n",
    "\n",
    "## 1. Theory\n",
    "\n",
    "The **Rectified Linear Unit (ReLU)** and its variants (**Leaky ReLU**, **Parametric ReLU**) are widely used activation functions in deep learning.\n",
    "\n",
    "- **ReLU**: Outputs the input directly if positive, otherwise outputs zero.\n",
    "- **Leaky ReLU**: Allows a small, non-zero gradient for negative inputs to prevent dying neurons.\n",
    "- **Parametric ReLU (PReLU)**: Similar to Leaky ReLU but the negative slope is a learnable parameter.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mathematical Formulas**\n",
    "\n",
    "#### ReLU:\n",
    "$$\n",
    "f(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "#### Leaky ReLU:\n",
    "$$\n",
    "f(x) =\n",
    "\\begin{cases}\n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha x & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where \\( \\alpha \\) is a small constant (e.g., 0.01).\n",
    "\n",
    "#### Parametric ReLU:\n",
    "$$\n",
    "f(x) =\n",
    "\\begin{cases}\n",
    "x & \\text{if } x > 0 \\\\\n",
    "a x & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where \\( a \\) is a **learnable parameter** instead of a fixed constant.\n",
    "\n",
    "---\n",
    "\n",
    "### **Derivatives**\n",
    "\n",
    "- For ReLU:\n",
    "\n",
    "$$\n",
    "f'(x) =\n",
    "\\begin{cases}\n",
    "1 & x > 0 \\\\\n",
    "0 & x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- For Leaky ReLU:\n",
    "\n",
    "$$\n",
    "f'(x) =\n",
    "\\begin{cases}\n",
    "1 & x > 0 \\\\\n",
    "\\alpha & x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- For Parametric ReLU:\n",
    "\n",
    "$$\n",
    "f'(x) =\n",
    "\\begin{cases}\n",
    "1 & x > 0 \\\\\n",
    "a & x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Graphical Intuition\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Advantages and Disadvantages\n",
    "\n",
    "| **Activation**     | **Advantages**                                                     | **Disadvantages**                                              |\n",
    "|---------------------|--------------------------------------------------------------------|----------------------------------------------------------------|\n",
    "| **ReLU**           | Simple, fast, avoids vanishing gradients for \\(x>0\\).             | Dying ReLU problem (neurons stuck at 0 for negative inputs).   |\n",
    "| **Leaky ReLU**     | Solves dying ReLU by allowing small gradients when \\(x<0\\).       | Slightly more computational cost.                              |\n",
    "| **Parametric ReLU**| Learns optimal slope for negative inputs during training.         | Adds extra parameters, increasing model complexity.           |\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Use Cases\n",
    "\n",
    "- **ReLU**: Default choice for most deep neural networks (CNNs, MLPs, ResNets).\n",
    "- **Leaky ReLU**: Preferred when dying ReLU is observed during training.\n",
    "- **Parametric ReLU**: Used in architectures where adaptive negative slopes improve performance (e.g., very deep CNNs).\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Interview Questions and Answers\n",
    "\n",
    "### **Q1: Why are Leaky ReLU and PReLU better than standard ReLU?**\n",
    "**Answer:**  \n",
    "- They prevent the dying ReLU problem by allowing a small gradient for negative inputs.\n",
    "- PReLU goes further by learning the slope parameter \\( a \\) during training, improving adaptability.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q2: When should you use PReLU over Leaky ReLU?**\n",
    "**Answer:**  \n",
    "- Use PReLU when you have sufficient data and computational resources, as it adds extra parameters to learn.\n",
    "- Use Leaky ReLU when you want a simple, fixed-slope alternative.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Conclusion\n",
    "ReLU and its variants are the **standard activations for hidden layers** in modern deep learning.  \n",
    "Leaky ReLU and PReLU **address the dying neuron issue**, making them more robust in certain scenarios.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169029c3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
