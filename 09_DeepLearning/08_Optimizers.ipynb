{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5e33ab8",
   "metadata": {},
   "source": [
    "# ðŸ”¹ Optimizers in Deep Learning â€“ Overview & Types\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Optimizers are algorithms used to **update the weights** of a neural network to **minimize the cost function**.  \n",
    "They determine **how** the model learns during training by adjusting weights based on gradients.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Why Optimizers are Important?\n",
    "\n",
    "- âœ… Control the **speed** of convergence.  \n",
    "- âœ… Prevent getting stuck in **local minima**.  \n",
    "- âœ… Handle challenges like **vanishing/exploding gradients**.  \n",
    "- âœ… Improve **generalization** and **model performance**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. How Optimizers Work?\n",
    "\n",
    "- Optimizers use **gradient information** from **Backpropagation** to update weights:\n",
    "  $$\n",
    "  w := w - \\eta \\frac{\\partial J}{\\partial w}\n",
    "  $$\n",
    "- Where:\n",
    "  - \\( w \\) â†’ weight\n",
    "  - \\( \\eta \\) â†’ learning rate\n",
    "  - \\( \\frac{\\partial J}{\\partial w} \\) â†’ gradient of cost function\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Types of Optimizers (Names Only)\n",
    "\n",
    "### âœ… **A. First-Order Optimizers (Gradient-Based)**\n",
    "1. **Gradient Descent (GD)**\n",
    "2. **Stochastic Gradient Descent (SGD)**\n",
    "3. **Mini-Batch Gradient Descent**\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **B. Gradient Descent Variants**\n",
    "4. **SGD with Momentum**\n",
    "5. **Nesterov Accelerated Gradient (NAG)**\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **C. Adaptive Learning Rate Optimizers**\n",
    "6. **AdaGrad** (Adaptive Gradient)\n",
    "7. **RMSProp** (Root Mean Square Propagation)\n",
    "8. **Adam** (Adaptive Moment Estimation)\n",
    "9. **AdaMax** (Variant of Adam)\n",
    "10. **Nadam** (Nesterov + Adam)\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **D. Advanced/Other Optimizers**\n",
    "11. **AMSGrad** (Improved Adam variant)\n",
    "12. **Lion Optimizer** (Latest efficient optimizer for transformers)\n",
    "13. **L-BFGS** (Quasi-Newton method, used in smaller networks)\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Conclusion\n",
    "- Optimizers are crucial for effective training.\n",
    "- **Adam** is the most widely used in deep learning.\n",
    "- Different optimizers are chosen based on **task, dataset size, and network architecture**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee43854",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
