{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1944796",
   "metadata": {},
   "source": [
    "## Tokenization in NLP using NLTK\n",
    "NLTK provides robust tokenization utilities that support both word-level and sentence-level tokenization based on rule-based and regular-expression-based methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85b34c2",
   "metadata": {},
   "source": [
    "### üîç Tokenization: Definition and Importance\n",
    "üìñ Definition:\n",
    "Tokenization is the process of converting a sequence of text (such as a sentence or paragraph) into smaller units called tokens. These tokens can be:\n",
    "\n",
    "Words (word-level tokenization)\n",
    "\n",
    "Subwords (subword-level tokenization)\n",
    "\n",
    "Characters (character-level tokenization)\n",
    "\n",
    "It is typically the first step in most NLP pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a16d16",
   "metadata": {},
   "source": [
    "### üß± Why Tokenization is Importan\n",
    "| Purpose                 | Description                                                             |\n",
    "| ----------------------- | ----------------------------------------------------------------------- |\n",
    "| **Input Structuring**   | Converts raw, unstructured text into a structured format for ML models. |\n",
    "| **Vocabulary Mapping**  | Helps build a vocabulary for embedding and model training.              |\n",
    "| **Feature Extraction**  | Enables computation of word frequencies, TF-IDF scores, and embeddings. |\n",
    "| **Contextual Analysis** | Necessary for POS tagging, NER, parsing, etc.                           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53ed058",
   "metadata": {},
   "source": [
    "### üß± Types of Tokenization in NLTK\n",
    "| Type                           | Method/Class              | Description                                                                     | Use Case                                                               |\n",
    "| ------------------------------ | ------------------------- | ------------------------------------------------------------------------------- | ---------------------------------------------------------------------- |\n",
    "| **1. Word Tokenization**       | `word_tokenize()`         | Splits a sentence into words and punctuation using the Penn Treebank tokenizer. | Standard word-level tokenization for most NLP tasks.                   |\n",
    "| **2. Sentence Tokenization**   | `sent_tokenize()`         | Splits a text into a list of sentences.                                         | Used in paragraph segmentation and sentence-level analysis.            |\n",
    "| **3. Regex Tokenization**      | `RegexpTokenizer()`       | Tokenizes text using custom regular expressions.                                | For structured/customized token patterns (e.g., removing punctuation). |\n",
    "| **4. Whitespace Tokenization** | `WhitespaceTokenizer()`   | Splits tokens based purely on whitespace.                                       | Useful when punctuation should be retained or for formatted data.      |\n",
    "| **5. WordPunct Tokenization**  | `WordPunctTokenizer()`    | Splits text into alphabetic and non-alphabetic characters.                      | Granular tokenization; separates words and punctuations distinctly.    |\n",
    "| **6. Treebank Tokenization**   | `TreebankWordTokenizer()` | Mimics Penn Treebank-style tokenization with specific rules.                    | For linguistically consistent and corpus-aligned NLP tasks.            |\n",
    "| **7. Blankline Tokenization**  | `BlanklineTokenizer()`    | Splits paragraphs based on blank lines.                                         | When dealing with multiple paragraphs separated by empty lines.        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77e469f",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1f2c8c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Ô∏è‚É£ Word Tokenizer: ['Hello', '!', 'I', \"'m\", 'Suraj', '.', 'I', 'work', 'in', 'A.I.', ',', 'and', 'I', 'love', 'NLP', '.', 'Do', \"n't\", 'you', '?', 'Let', \"'s\", 'tokenize', 'this', 'text', '.']\n",
      "2Ô∏è‚É£ Sentence Tokenizer: ['Hello!', \"I'm Suraj.\", 'I work in A.I., and I love NLP.', \"Don't you?\", \"Let's tokenize this text.\"]\n",
      "3Ô∏è‚É£ Regex Tokenizer: ['Hello', 'I', 'm', 'Suraj', 'I', 'work', 'in', 'A', 'I', 'and', 'I', 'love', 'NLP', 'Don', 't', 'you', 'Let', 's', 'tokenize', 'this', 'text']\n",
      "4Ô∏è‚É£ Whitespace Tokenizer: ['Hello!', \"I'm\", 'Suraj.', 'I', 'work', 'in', 'A.I.,', 'and', 'I', 'love', 'NLP.', \"Don't\", 'you?', \"Let's\", 'tokenize', 'this', 'text.']\n",
      "5Ô∏è‚É£ WordPunct Tokenizer: ['Hello', '!', 'I', \"'\", 'm', 'Suraj', '.', 'I', 'work', 'in', 'A', '.', 'I', '.,', 'and', 'I', 'love', 'NLP', '.', 'Don', \"'\", 't', 'you', '?', 'Let', \"'\", 's', 'tokenize', 'this', 'text', '.']\n",
      "6Ô∏è‚É£ Treebank Word Tokenizer: ['Hello', '!', 'I', \"'m\", 'Suraj.', 'I', 'work', 'in', 'A.I.', ',', 'and', 'I', 'love', 'NLP.', 'Do', \"n't\", 'you', '?', 'Let', \"'s\", 'tokenize', 'this', 'text', '.']\n",
      "7Ô∏è‚É£ Blankline Tokenizer: [\"Hello! I'm Suraj.\", 'I work in A.I., and I love NLP.', \"Don't you?\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Suraj\n",
      "[nltk_data]     Khodade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import (\n",
    "    word_tokenize, sent_tokenize, RegexpTokenizer,\n",
    "    WhitespaceTokenizer, WordPunctTokenizer,\n",
    "    TreebankWordTokenizer, BlanklineTokenizer\n",
    ")\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"Hello! I'm Suraj. I work in A.I., and I love NLP. Don't you? Let's tokenize this text.\"\n",
    "\n",
    "# 1. Word Tokenizer\n",
    "word_tok = word_tokenize(text)\n",
    "\n",
    "# 2. Sentence Tokenizer\n",
    "sent_tok = sent_tokenize(text)\n",
    "\n",
    "# 3. Regex Tokenizer (alphanumeric only)\n",
    "regex_tok = RegexpTokenizer(r'\\w+').tokenize(text)\n",
    "\n",
    "# 4. Whitespace Tokenizer\n",
    "whitespace_tok = WhitespaceTokenizer().tokenize(text)\n",
    "\n",
    "# 5. WordPunct Tokenizer\n",
    "word_punct_tok = WordPunctTokenizer().tokenize(text)\n",
    "\n",
    "# 6. Treebank Word Tokenizer\n",
    "treebank_tok = TreebankWordTokenizer().tokenize(text)\n",
    "\n",
    "# 7. Blankline Tokenizer (used for multi-paragraph)\n",
    "paragraph_text = \"Hello! I'm Suraj.\\n\\nI work in A.I., and I love NLP.\\n\\nDon't you?\"\n",
    "blankline_tok = BlanklineTokenizer().tokenize(paragraph_text)\n",
    "\n",
    "# Print results\n",
    "print(\"1Ô∏è‚É£ Word Tokenizer:\", word_tok)\n",
    "print(\"2Ô∏è‚É£ Sentence Tokenizer:\", sent_tok)\n",
    "print(\"3Ô∏è‚É£ Regex Tokenizer:\", regex_tok)\n",
    "print(\"4Ô∏è‚É£ Whitespace Tokenizer:\", whitespace_tok)\n",
    "print(\"5Ô∏è‚É£ WordPunct Tokenizer:\", word_punct_tok)\n",
    "print(\"6Ô∏è‚É£ Treebank Word Tokenizer:\", treebank_tok)\n",
    "print(\"7Ô∏è‚É£ Blankline Tokenizer:\", blankline_tok)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2820a1fd",
   "metadata": {},
   "source": [
    "### üìä Comparison of Outputs\n",
    "| Tokenizer                      | Output                                                                                                                                                 | Notes                                                                    |\n",
    "| ------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------ |\n",
    "| **1. word\\_tokenize()**        | `['Hello', '!', 'I', \"'m\", 'Suraj', '.', 'I', 'work', 'in', 'A.I.', ',', 'and', 'I', 'love', 'NLP', '.', 'Do', \"n't\", 'you', '?']`                     | Accurate, handles contractions & punctuations.                           |\n",
    "| **2. sent\\_tokenize()**        | `['Hello!', \"I'm Suraj.\", 'I work in A.I., and I love NLP.', \"Don't you?\"]`                                                                            | Segments text into sentences.                                            |\n",
    "| **3. RegexpTokenizer(r'\\w+')** | `['Hello', 'I', 'm', 'Suraj', 'I', 'work', 'in', 'A', 'I', 'and', 'I', 'love', 'NLP', 'Don', 't', 'you']`                                              | Removes punctuation; breaks contractions and abbreviations.              |\n",
    "| **4. WhitespaceTokenizer()**   | `['Hello!', \"I'm\", 'Suraj.', 'I', 'work', 'in', 'A.I.,', 'and', 'I', 'love', 'NLP.', \"Don't\", 'you?']`                                                 | Preserves punctuation, splits only on spaces.                            |\n",
    "| **5. WordPunctTokenizer()**    | `['Hello', '!', 'I', \"'\", 'm', 'Suraj', '.', 'I', 'work', 'in', 'A', '.', 'I', '.', ',', 'and', 'I', 'love', 'NLP', '.', 'Don', \"'\", 't', 'you', '?']` | Separates everything aggressively; breaks punctuations and contractions. |\n",
    "| **6. TreebankWordTokenizer()** | `['Hello', '!', 'I', \"'m\", 'Suraj', '.', 'I', 'work', 'in', 'A.I.', ',', 'and', 'I', 'love', 'NLP', '.', 'Do', \"n't\", 'you', '?']`                     | Matches word\\_tokenize behavior (since it‚Äôs used internally).            |\n",
    "| **7. BlanklineTokenizer()**    | `[\"Hello! I'm Suraj.\", 'I work in A.I., and I love NLP.', \"Don't you?\"]`                                                                               | Splits text into paragraphs based on blank lines.                        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4c706012",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"Hello ! I am Suraj Khodade.. It's my git repo for AIBootcamp. I am a Data Scientist, Machine Learning Engineer. I love to work on NLP projects. I am currently learning about tokenization in NLP using NLTK library.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0b32f134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello ! I am Suraj Khodade.. It's my git repo for AIBootcamp. I am a Data Scientist, Machine Learning Engineer. I love to work on NLP projects. I am currently learning about tokenization in NLP using NLTK library.\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5f4bfa6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello ! I am Suraj Khodade.. It's my git repo for AIBootcamp. I am a Data Scientist, Machine Learning Engineer. I love to work on NLP projects. I am currently learning about tokenization in NLP using NLTK library.\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0989e296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "152bf45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello !',\n",
       " 'I am Suraj Khodade..',\n",
       " \"It's my git repo for AIBootcamp.\",\n",
       " 'I am a Data Scientist, Machine Learning Engineer.',\n",
       " 'I love to work on NLP projects.',\n",
       " 'I am currently learning about tokenization in NLP using NLTK library.']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = sent_tokenize(corpus, language='english')\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e5c8339a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "50cf1888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello !\n",
      "I am Suraj Khodade..\n",
      "It's my git repo for AIBootcamp.\n",
      "I am a Data Scientist, Machine Learning Engineer.\n",
      "I love to work on NLP projects.\n",
      "I am currently learning about tokenization in NLP using NLTK library.\n"
     ]
    }
   ],
   "source": [
    "for sentense in document:\n",
    "    print(sentense) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "32ac9849",
   "metadata": {},
   "outputs": [],
   "source": [
    "## word tokenization\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5b4573c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " '!',\n",
       " 'I',\n",
       " 'am',\n",
       " 'Suraj',\n",
       " 'Khodade',\n",
       " '..',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'my',\n",
       " 'git',\n",
       " 'repo',\n",
       " 'for',\n",
       " 'AIBootcamp',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'a',\n",
       " 'Data',\n",
       " 'Scientist',\n",
       " ',',\n",
       " 'Machine',\n",
       " 'Learning',\n",
       " 'Engineer',\n",
       " '.',\n",
       " 'I',\n",
       " 'love',\n",
       " 'to',\n",
       " 'work',\n",
       " 'on',\n",
       " 'NLP',\n",
       " 'projects',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'currently',\n",
       " 'learning',\n",
       " 'about',\n",
       " 'tokenization',\n",
       " 'in',\n",
       " 'NLP',\n",
       " 'using',\n",
       " 'NLTK',\n",
       " 'library',\n",
       " '.']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_document = word_tokenize(corpus, language='english')\n",
    "word_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "684db288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hello', '!'],\n",
       " ['I', 'am', 'Suraj', 'Khodade', '..'],\n",
       " ['It', \"'s\", 'my', 'git', 'repo', 'for', 'AIBootcamp', '.'],\n",
       " ['I',\n",
       "  'am',\n",
       "  'a',\n",
       "  'Data',\n",
       "  'Scientist',\n",
       "  ',',\n",
       "  'Machine',\n",
       "  'Learning',\n",
       "  'Engineer',\n",
       "  '.'],\n",
       " ['I', 'love', 'to', 'work', 'on', 'NLP', 'projects', '.'],\n",
       " ['I',\n",
       "  'am',\n",
       "  'currently',\n",
       "  'learning',\n",
       "  'about',\n",
       "  'tokenization',\n",
       "  'in',\n",
       "  'NLP',\n",
       "  'using',\n",
       "  'NLTK',\n",
       "  'library',\n",
       "  '.']]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_sentence : list = [] \n",
    "for sentense in document:\n",
    "    word_sentence.append(word_tokenize(sentense, language='english'))\n",
    "\n",
    "word_sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "28cdf3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "!\n",
      "I\n",
      "am\n",
      "Suraj\n",
      "Khodade\n",
      "..\n",
      "It\n",
      "'s\n",
      "my\n",
      "git\n",
      "repo\n",
      "for\n",
      "AIBootcamp\n",
      ".\n",
      "I\n",
      "am\n",
      "a\n",
      "Data\n",
      "Scientist\n",
      ",\n",
      "Machine\n",
      "Learning\n",
      "Engineer\n",
      ".\n",
      "I\n",
      "love\n",
      "to\n",
      "work\n",
      "on\n",
      "NLP\n",
      "projects\n",
      ".\n",
      "I\n",
      "am\n",
      "currently\n",
      "learning\n",
      "about\n",
      "tokenization\n",
      "in\n",
      "NLP\n",
      "using\n",
      "NLTK\n",
      "library\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for word in word_document:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "273c449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## word punctuation tokenization\n",
    "## used to split words and punctuation\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "aa5dcc83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " '!',\n",
       " 'I',\n",
       " 'am',\n",
       " 'Suraj',\n",
       " 'Khodade',\n",
       " '..',\n",
       " 'It',\n",
       " \"'\",\n",
       " 's',\n",
       " 'my',\n",
       " 'git',\n",
       " 'repo',\n",
       " 'for',\n",
       " 'AIBootcamp',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'a',\n",
       " 'Data',\n",
       " 'Scientist',\n",
       " ',',\n",
       " 'Machine',\n",
       " 'Learning',\n",
       " 'Engineer',\n",
       " '.',\n",
       " 'I',\n",
       " 'love',\n",
       " 'to',\n",
       " 'work',\n",
       " 'on',\n",
       " 'NLP',\n",
       " 'projects',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'currently',\n",
       " 'learning',\n",
       " 'about',\n",
       " 'tokenization',\n",
       " 'in',\n",
       " 'NLP',\n",
       " 'using',\n",
       " 'NLTK',\n",
       " 'library',\n",
       " '.']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_document = wordpunct_tokenize(corpus)\n",
    "word_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fa726064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "## treebank word tokenizer used to tokenize words in a way that is similar to the Penn Treebank\n",
    "## different from wordpunct_tokenize as it does not split punctuation\n",
    "## full stops are not considered as separate tokens except at the end of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fd0efaab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " '!',\n",
       " 'I',\n",
       " 'am',\n",
       " 'Suraj',\n",
       " 'Khodade..',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'my',\n",
       " 'git',\n",
       " 'repo',\n",
       " 'for',\n",
       " 'AIBootcamp.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'a',\n",
       " 'Data',\n",
       " 'Scientist',\n",
       " ',',\n",
       " 'Machine',\n",
       " 'Learning',\n",
       " 'Engineer.',\n",
       " 'I',\n",
       " 'love',\n",
       " 'to',\n",
       " 'work',\n",
       " 'on',\n",
       " 'NLP',\n",
       " 'projects.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'currently',\n",
       " 'learning',\n",
       " 'about',\n",
       " 'tokenization',\n",
       " 'in',\n",
       " 'NLP',\n",
       " 'using',\n",
       " 'NLTK',\n",
       " 'library',\n",
       " '.']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_treebank = TreebankWordTokenizer()\n",
    "word_treebank.tokenize(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c70107",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb9e9b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
