{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17555ccf",
   "metadata": {},
   "source": [
    "# üß† Sigmoid Activation Function\n",
    "\n",
    "## 1. Theory\n",
    "\n",
    "The **Sigmoid Activation Function** is a non-linear function that maps any real-valued number into the range (0,1).  \n",
    "It is widely used in early neural networks to introduce non-linearity and to model probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mathematical Formula**\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "- For large positive \\( x \\), \\( \\sigma(x) \\to 1 \\)\n",
    "- For large negative \\( x \\), \\( \\sigma(x) \\to 0 \\)\n",
    "- For \\( x = 0 \\), \\( \\sigma(0) = 0.5 \\)\n",
    "\n",
    "---\n",
    "\n",
    "### **Derivative**\n",
    "\n",
    "The derivative of the sigmoid function is:\n",
    "\n",
    "$$\n",
    "\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))\n",
    "$$\n",
    "\n",
    "- This derivative is **maximum at \\( x = 0 \\)** and equals 0.25\n",
    "- For very large or very small \\( x \\), the derivative approaches 0 (causing vanishing gradients)\n",
    "The function formula and chart are as follows\n",
    "\n",
    "![alt](img/sig.svg)\n",
    "\n",
    "![alt](img/2.png)\n",
    "---\n",
    "\n",
    "## 2. Graphical Intuition\n",
    "\n",
    "The sigmoid curve is S-shaped:\n",
    "\n",
    "\n",
    "## 3. Advantages and Disadvantages\n",
    "\n",
    "| **Aspect**          | **Advantages**                                                   | **Disadvantages**                                               |\n",
    "|---------------------|------------------------------------------------------------------|----------------------------------------------------------------|\n",
    "| **Range**           | Maps input to (0,1), useful for probability interpretation.      | Saturates at extremes, causing vanishing gradients.            |\n",
    "| **Smoothness**      | Differentiable everywhere, enabling gradient-based learning.     | Output is not zero-centered, leading to zig-zagging updates.   |\n",
    "| **Non-linearity**   | Allows networks to learn non-linear decision boundaries.         | Computationally expensive (requires exponentials).             |\n",
    "| **Interpretability**| Outputs can be treated as probabilities in binary classification.| Not ideal for deep networks (ReLU is preferred).               |\n",
    "\n",
    "---\n",
    "Advantages of Sigmoid Function : -\n",
    "\n",
    "1. Smooth gradient, preventing ‚Äújumps‚Äù in output values.\n",
    "2. Output values bound between 0 and 1, normalizing the output of each neuron.\n",
    "3. Clear predictions, i.e very close to 1 or 0.\n",
    "\n",
    "\n",
    "Sigmoid has three major disadvantages:\n",
    "* Prone to gradient vanishing\n",
    "* Function output is not zero-centered\n",
    "* Power operations are relatively time consuming\n",
    "## 4. Use Cases\n",
    "\n",
    "The sigmoid activation is primarily used in:\n",
    "\n",
    "1. **Binary Classification**  \n",
    "   - Output layer of logistic regression  \n",
    "   - Output neuron in binary classification neural networks\n",
    "\n",
    "2. **Probabilistic Models**  \n",
    "   - Models where output represents a probability score\n",
    "\n",
    "3. **Gating Mechanisms**  \n",
    "   - Used in gates of LSTM (Long Short-Term Memory) networks (though with modifications)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Conclusion\n",
    "The **Sigmoid** activation is simple and interpretable, making it effective for **binary classification** tasks.  \n",
    "However, it is prone to the **vanishing gradient problem**, so modern deep networks prefer **ReLU** or its variants for hidden layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd35015",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
