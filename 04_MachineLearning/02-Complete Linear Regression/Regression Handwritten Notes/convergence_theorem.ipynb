{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "698585b0",
   "metadata": {},
   "source": [
    "### Convergence Theorems in Machine Learning\n",
    "Convergence theorems in ML ensure that learning algorithms stabilize as they iterate, and that learned models approximate the true underlying function or distribution over time and with more data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dd586f",
   "metadata": {},
   "source": [
    "###   Key Convergence Theorems in ML Context\n",
    "| **Theorem / Principle**                           | **Description**                                                                                                             | **Relevance in ML**                                                    |\n",
    "| ------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------- |\n",
    "| **Empirical Risk Minimization (ERM) Convergence** | Under i.i.d. data and a fixed hypothesis class, the empirical risk converges to the expected risk as sample size grows.     | Justifies training on finite datasets to generalize well.              |\n",
    "| **Uniform Convergence Theorem**                   | With high probability, the worst-case deviation between empirical and expected risk over a class of functions is small.     | Foundation for VC dimension, generalization bounds.                    |\n",
    "| **Gradient Descent Convergence Theorem**          | For convex functions with Lipschitz-continuous gradients, gradient descent converges to the global minimum at a known rate. | Guarantees convergence of training loss in deep learning.              |\n",
    "| **Stochastic Gradient Descent (SGD) Convergence** | Under bounded variance and diminishing learning rate, SGD converges in expectation to a local/global minimum.               | Core to online learning and training large models.                     |\n",
    "| **Law of Large Numbers (LLN)**                    | Sample averages converge to expected values as the number of samples increases.                                             | Justifies model training via empirical loss minimization.              |\n",
    "| **Central Limit Theorem (CLT)**                   | Distribution of sample means approaches normality as sample size increases.                                                 | Underpins confidence intervals and hypothesis tests in ML evaluations. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc03d55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: x = 0.6000, f(x) = 5.760000\n",
      "Iter 10: x = 2.7423, f(x) = 0.066408\n",
      "Iter 20: x = 2.9723, f(x) = 0.000766\n",
      "Iter 30: x = 2.9970, f(x) = 0.000009\n",
      "Iter 40: x = 2.9997, f(x) = 0.000000\n",
      "Iter 50: x = 3.0000, f(x) = 0.000000\n",
      "Iter 60: x = 3.0000, f(x) = 0.000000\n",
      "Iter 70: x = 3.0000, f(x) = 0.000000\n",
      "Iter 80: x = 3.0000, f(x) = 0.000000\n",
      "Iter 90: x = 3.0000, f(x) = 0.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Objective: minimize f(x) = (x - 3)^2\n",
    "def f(x): return (x - 3) ** 2\n",
    "def grad_f(x): return 2 * (x - 3)\n",
    "\n",
    "x = 0  # initial point\n",
    "lr = 0.1  # learning rate\n",
    "for i in range(100):\n",
    "    grad = grad_f(x)\n",
    "    x = x - lr * grad  # SGD step\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Iter {i}: x = {x:.4f}, f(x) = {f(x):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1bfc60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
