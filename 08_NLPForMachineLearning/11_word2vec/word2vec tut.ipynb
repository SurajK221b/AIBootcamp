{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c7e9fb9",
   "metadata": {},
   "source": [
    "## üîπ 1. Word Embedding \n",
    "Definition:\n",
    "A word embedding is a dense vector representation of words in a continuous vector space, capturing semantic and syntactic similarities. Unlike one-hot encoding, word embeddings allow us to compute similarity and relationships between words.\n",
    "\n",
    "Popular techniques:\n",
    "\n",
    "Word2Vec (CBOW, Skip-Gram)\n",
    "\n",
    "GloVe (Global Vectors)\n",
    "\n",
    "FastText\n",
    "\n",
    "BERT embeddings (contextual)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab828d2",
   "metadata": {},
   "source": [
    "### üîπ 2. Word2Vec\n",
    "Developed by Google in 2013, Word2Vec converts words into vector form using shallow neural networks. The objective is to predict context from a word or a word from context.\n",
    "\n",
    "#### It has two architectures:\n",
    "1. CBOW (Continuous Bag of Words)\n",
    "2. Skip-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3842734",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7229f5b4",
   "metadata": {},
   "source": [
    "### üìò CBOW (Continuous Bag of Words) - Theoretical Overview\n",
    "üîπ Definition\n",
    "CBOW is a neural network-based architecture introduced as part of the Word2Vec framework. Its primary objective is to predict a target (central) word based on its context words (surrounding words within a predefined window). It treats the context as a bag of words‚Äîignoring word order, hence the name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc1ece6",
   "metadata": {},
   "source": [
    "### ‚úÖ Advantages of CBOW\n",
    "| Advantage                          | Explanation                                                           |\n",
    "| ---------------------------------- | --------------------------------------------------------------------- |\n",
    "| **Efficiency**                     | Faster training due to fewer parameters and simpler architecture      |\n",
    "| **Good for Small Datasets**        | Performs well when data is limited or less noisy                      |\n",
    "| **Deterministic Context**          | Predicting a single target from known context simplifies optimization |\n",
    "| **Less Computationally Intensive** | Compared to Skip-Gram, CBOW uses less compute power                   |\n",
    "\n",
    "\n",
    "### ‚ö†Ô∏è Disadvantages of CBOW\n",
    "| Disadvantage                     | Explanation                                                                                  |\n",
    "| -------------------------------- | -------------------------------------------------------------------------------------------- |\n",
    "| **Ignores Word Order**           | The model uses a bag of words approach‚Äîsemantic meaning from word order is lost              |\n",
    "| **Not Ideal for Rare Words**     | CBOW averages context, which can dilute signals from infrequent or informative context words |\n",
    "| **Context Window Sensitivity**   | Performance heavily depends on the chosen window size                                        |\n",
    "| **Cannot Capture Polysemy Well** | Assigns a single vector to each word, ignoring multiple meanings (e.g., ‚Äúbank‚Äù)              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18c1497",
   "metadata": {},
   "source": [
    "### üìò Skip-Gram ‚Äì Theoretical Overview\n",
    "\n",
    "üîπ Definition\n",
    "Skip-Gram is one of the two primary architectures in the Word2Vec framework (alongside CBOW). Unlike CBOW, which predicts a target word based on its surrounding context, Skip-Gram predicts surrounding context words given a central target word.\n",
    "\n",
    "### ‚úÖ Advantages of Skip-Gram\n",
    "| Advantage                           | Description                                                                                            |\n",
    "| ----------------------------------- | ------------------------------------------------------------------------------------------------------ |\n",
    "| **Effective for Rare Words**        | Performs well for infrequent words due to independent context predictions                              |\n",
    "| **Better Semantic Representation**  | Captures syntactic and semantic relationships more accurately than CBOW                                |\n",
    "| **Flexible Context Modeling**       | Predicts multiple context words, enhancing richness of training signals                                |\n",
    "| **Scalable with Negative Sampling** | Efficiently trains on large corpora using optimizations like negative sampling or hierarchical softmax |\n",
    "\n",
    "\n",
    "### ‚ö†Ô∏è Disadvantages of Skip-Gram\n",
    "| Disadvantage                  | Description                                                                 |\n",
    "| ----------------------------- | --------------------------------------------------------------------------- |\n",
    "| **Computationally Intensive** | Predicts multiple context words ‚Üí more forward and backward passes per word |\n",
    "| **Longer Training Time**      | Especially with large vocabularies and wide context windows                 |\n",
    "| **Requires Large Corpus**     | Performs best with large-scale textual data (e.g., Wikipedia, Google News)  |\n",
    "| **Ignores Word Order**        | Like CBOW, it assumes a bag of context, ignoring sequence position          |\n",
    "\n",
    "### üîÅ CBOW vs Skip-Gram Summary\n",
    "| Feature               | CBOW           | Skip-Gram     |\n",
    "| --------------------- | -------------- | ------------- |\n",
    "| Input                 | Context words  | Target word   |\n",
    "| Output                | Target word    | Context words |\n",
    "| Speed                 | Faster         | Slower        |\n",
    "| Accuracy (rare words) | Lower          | Higher        |\n",
    "| Best For              | Small datasets | Large corpora |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8a30d3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
